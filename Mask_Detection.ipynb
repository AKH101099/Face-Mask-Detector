{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import random\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detector Using Tensorflow and OpenCV/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of With mask vs Without Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Mask:  690\n",
      "Without Mask:  686\n"
     ]
    }
   ],
   "source": [
    "print(\"With Mask: \",len(os.listdir('Images/with_mask')))\n",
    "print(\"Without Mask: \",len(os.listdir('Images/without_mask')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAIN, TEST, SPLIT_SIZE):\n",
    "    proper_data = []\n",
    "    for img in os.listdir(SOURCE):\n",
    "        img_path = SOURCE + img\n",
    "        if(os.path.getsize(img_path)>0):\n",
    "            proper_data.append(img)\n",
    "        else:\n",
    "            print(\"Issue in the Image:\"+ img_path)\n",
    "            \n",
    "    training_size = int(len(proper_data)*SPLIT_SIZE)\n",
    "    shuffled_data = random.sample(proper_data, len(proper_data))\n",
    "    training_data = shuffled_data[:training_size]\n",
    "    testing_data = shuffled_data[training_size:]\n",
    "    #print(training_data)\n",
    "    for img in training_data:\n",
    "        temp_img = SOURCE + img\n",
    "        train_img = TRAIN + img\n",
    "        copyfile(temp_img, train_img)\n",
    "\n",
    "    for img in testing_data:\n",
    "        temp_img = SOURCE + img\n",
    "        test_img = TEST + img\n",
    "        copyfile(temp_img, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_Source = 'Images/with_mask/'\n",
    "TRAINING_YES_Source = 'Images/train/with_mask/'\n",
    "TESTING_YES_Source = 'Images/test/with_mask/'\n",
    "NO_Source = 'Images/without_mask/'\n",
    "TRAINING_NO_Source = 'Images/train/without_mask/'\n",
    "TESTING_NO_Source = 'Images/test/without_mask/'\n",
    "split_size = 0.8\n",
    "split_data(YES_Source, TRAINING_YES_Source, TESTING_YES_Source, split_size)\n",
    "split_data(NO_Source, TRAINING_NO_Source, TESTING_NO_Source, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training with mask:  552\n",
      "Number of training without mask:  548\n",
      "Number of testing with mask:  138\n",
      "Number of testing without mask:  138\n"
     ]
    }
   ],
   "source": [
    "print('Number of training with mask: ', len(os.listdir(TRAINING_YES_Source)))\n",
    "print('Number of training without mask: ', len(os.listdir(TRAINING_NO_Source)))\n",
    "print('Number of testing with mask: ', len(os.listdir(TESTING_YES_Source)))\n",
    "print('Number of testing without mask: ', len(os.listdir(TESTING_NO_Source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN Model using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anand Gaurav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Creating the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 images belonging to 2 classes.\n",
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'Images/train/'\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.0,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                    target_size = (150,150),\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size = 32)\n",
    "TEST_DIR = 'Images/test/'\n",
    "test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
    "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
    "                                                target_size = (150,150),\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to get best weights used when monitored with validation loss\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.1600 - acc: 0.9464 - val_loss: 0.1062 - val_acc: 0.9783\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2422 - acc: 0.9282 - val_loss: 0.1133 - val_acc: 0.9746\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2013 - acc: 0.9427 - val_loss: 0.0851 - val_acc: 0.9783\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 48s 1s/step - loss: 0.1638 - acc: 0.9409 - val_loss: 0.0849 - val_acc: 0.9783\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.1251 - acc: 0.9527 - val_loss: 0.1486 - val_acc: 0.9674\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.1871 - acc: 0.9373 - val_loss: 0.0858 - val_acc: 0.9710\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.1368 - acc: 0.9482 - val_loss: 0.0953 - val_acc: 0.9601\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.1149 - acc: 0.9600 - val_loss: 0.0986 - val_acc: 0.9746\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.1301 - acc: 0.9591 - val_loss: 0.0773 - val_acc: 0.9746\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 57s 2s/step - loss: 0.1081 - acc: 0.9591 - val_loss: 0.0609 - val_acc: 0.9855\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.0863 - acc: 0.9655 - val_loss: 0.0904 - val_acc: 0.9746\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.0931 - acc: 0.9700 - val_loss: 0.1046 - val_acc: 0.9746\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.1070 - acc: 0.9582 - val_loss: 0.1125 - val_acc: 0.9565\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 48s 1s/step - loss: 0.0839 - acc: 0.9700 - val_loss: 0.0770 - val_acc: 0.9783\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.1148 - acc: 0.9673 - val_loss: 0.0646 - val_acc: 0.9855\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.0815 - acc: 0.9736 - val_loss: 0.0660 - val_acc: 0.9819\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.0914 - acc: 0.9700 - val_loss: 0.0870 - val_acc: 0.9819\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 59s 2s/step - loss: 0.1271 - acc: 0.9600 - val_loss: 0.0638 - val_acc: 0.9746\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 59s 2s/step - loss: 0.0629 - acc: 0.9773 - val_loss: 0.0622 - val_acc: 0.9855\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.0576 - acc: 0.9845 - val_loss: 0.0957 - val_acc: 0.9783\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 60s 2s/step - loss: 0.0627 - acc: 0.9809 - val_loss: 0.0993 - val_acc: 0.9746\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.0783 - acc: 0.9700 - val_loss: 0.0511 - val_acc: 0.9819\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.0682 - acc: 0.9755 - val_loss: 0.0614 - val_acc: 0.9746\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.0541 - acc: 0.9800 - val_loss: 0.0604 - val_acc: 0.9819\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0693 - acc: 0.9782 - val_loss: 0.0733 - val_acc: 0.9855\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0648 - acc: 0.9773 - val_loss: 0.0711 - val_acc: 0.9746\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0642 - acc: 0.9809 - val_loss: 0.0879 - val_acc: 0.9819\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0540 - acc: 0.9809 - val_loss: 0.0692 - val_acc: 0.9855\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0715 - acc: 0.9782 - val_loss: 0.0689 - val_acc: 0.9891\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.0590 - acc: 0.9809 - val_loss: 0.0793 - val_acc: 0.9855\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                             epochs = 30,\n",
    "                             validation_data=test_generator,\n",
    "                             callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove the commented Code lines to save your Video'''\n",
    "\n",
    "labels = {0:'No Mask, Please wear it!',1:'Mask On, Good work!'}\n",
    "color = {0:(0,0,255), 1:(255,0,0)}\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'ADV1')\n",
    "#out = cv2.VideoWriter('lol.mp4', fourcc, 25, (640,480))\n",
    "while webcam.isOpened():\n",
    "    _, frame = webcam.read()\n",
    "    frame = cv2.flip(frame, 1, 1)\n",
    "    faces = classifier.detectMultiScale(frame, 1.1, 4)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        #cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 3)\n",
    "        face_data = frame[y:y+h, x:x+w]\n",
    "        resize_data = cv2.resize(face_data, (150,150))\n",
    "        resize_data = resize_data/255.0\n",
    "        final_data = np.expand_dims(resize_data, axis = 0)\n",
    "        prediction = model.predict(final_data)\n",
    "        \n",
    "        binary_answer = np.argmax(prediction,axis=1)[0]\n",
    "        \n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), color[binary_answer], 3)\n",
    "        cv2.rectangle(frame, (x,y-40), (x+w, y), color[binary_answer], -1)\n",
    "        cv2.putText(frame, labels[binary_answer], (x,y-10), cv2.FONT_HERSHEY_PLAIN, 1, (255,255,255), 1)\n",
    "    \n",
    "    cv2.imshow(\"CORONA MASK\", frame)\n",
    "    #out.write(frame)\n",
    "        \n",
    "    if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "#out.release()\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
